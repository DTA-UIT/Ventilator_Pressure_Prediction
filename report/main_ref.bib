@misc{advantage_XGBoost, 
title={Machine Learning Algorithms: A comparison of different algorithms and when to use them}, 
url={https://medium.com/@taniyaghosh29/machine-learning-algorithms-what-are-the-differences-9b71df4f248f}, author={Taniya}, year={2018}, month={May}}

@misc{An-xgboost,
title={XGBoost: A Deep Dive into Boosting}, 
url={https://medium.com/sfu-cspmp/xgboost-a-deep-dive-into-boosting-f06c9c41349},
author={Rohan Harode},
year={2020},
month={February},
day={4}
}

@article{catboost,
  title={CatBoost: gradient boosting with categorical features support},
  author={Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
  journal={arXiv preprint arXiv:1810.11363},
  year={2018}
}
@misc{ggbrain, 
title={Google Brain - Ventilator Pressure Prediction}, 
url={https://www.kaggle.com/c/ventilator-pressure-prediction/overview/evaluation}, author={Google Brain}, year={2021}, month={Nov}}

@misc{sota-xgboost,
      title={Robust LogitBoost and Adaptive Base Class (ABC) LogitBoost}, 
      author={Ping Li},
      year={2012},
      eprint={1203.3491},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{example1,
author = {He, Xinran and Pan, Junfeng and Jin, Ou and Xu, Tianbing and Liu, Bo and Xu, Tao and Shi, Yanxin and Atallah, Antoine and Herbrich, Ralf and Bowers, Stuart and Candela, Joaquin Qui\~{n}onero},
title = {Practical Lessons from Predicting Clicks on Ads at Facebook},
year = {2014},
isbn = {9781450329996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648584.2648589},
doi = {10.1145/2648584.2648589},
abstract = {Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central to most online advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3\%, an improvement with significant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surprisingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic regression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with.},
booktitle = {Proceedings of the Eighth International Workshop on Data Mining for Online Advertising},
pages = {1â€“9},
numpages = {9},
location = {New York, NY, USA},
series = {ADKDD'14}
}

@article{xgboost,
   title={XGBoost},
   url={http://dx.doi.org/10.1145/2939672.2939785},
   DOI={10.1145/2939672.2939785},
   journal={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   publisher={ACM},
   author={Chen, Tianqi and Guestrin, Carlos},
   year={2016},
   month={Aug}
}

@incollection{adaboost,
  title={Explaining adaboost},
  author={Schapire, Robert E},
  booktitle={Empirical inference},
  pages={37--52},
  year={2013},
  publisher={Springer}
}
@article{svm,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}
@article{logisticregression,
  title={The regression analysis of binary sequences},
  author={Cox, David R},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={20},
  number={2},
  pages={215--232},
  year={1958},
  publisher={Wiley Online Library}
}

@article{kfold,
  title={Cross-validatory choice and assessment of statistical predictions},
  author={Stone, Mervyn},
  journal={Journal of the royal statistical society: Series B (Methodological)},
  volume={36},
  number={2},
  pages={111--133},
  year={1974},
  publisher={Wiley Online Library}
}
@article{ANN,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{gradient-boosting,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@article{gridsearch,
  title={On the relationship between classical grid search and probabilistic roadmaps},
  author={LaValle, Steven M and Branicky, Michael S and Lindemann, Stephen R},
  journal={The International Journal of Robotics Research},
  volume={23},
  number={7-8},
  pages={673--692},
  year={2004},
  publisher={SAGE Publications}
}

@article{lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={3146--3154},
  year={2017}
}

@inproceedings{randomforest,
  title={Random decision forests},
  author={Ho, Tin Kam},
  booktitle={Proceedings of 3rd international conference on document analysis and recognition},
  volume={1},
  pages={278--282},
  year={1995},
  organization={IEEE}
}

@book{ensemblelearning,
author = {Zhang, Cha and Ma, Yunqian},
title = {Ensemble Machine Learning: Methods and Applications},
year = {2012},
isbn = {1441993258},
publisher = {Springer Publishing Company, Incorporated},
abstract = {It is common wisdom that gathering a variety of views and inputs improves the process of decision making, and, indeed, underpins a democratic society. Dubbed ensemble learning by researchers in computational intelligence and machine learning, it is known to improve a decision systems robustness and accuracy. Now, fresh developments are allowing researchers to unleash the power of ensemble learning in an increasing range of real-world applications. Ensemble learning algorithms such as boosting and random forest facilitate solutions to key computational issues such as face recognition and are now being applied in areas as diverse as object tracking and bioinformatics. Responding to a shortage of literature dedicated to the topic, this volume offers comprehensive coverage of state-of-the-art ensemble learning techniques, including the random forest skeleton tracking algorithm in the Xbox Kinect sensor, which bypasses the need for game controllers. At once a solid theoretical study and a practical guide, the volume is a windfall for researchers and practitioners alike.}
}